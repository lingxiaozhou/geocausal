---
title: "Step 2. Smoothing point process data"
output: 
  rmarkdown::html_vignette:
    toc: true
    number_sections: true
vignette: >
  %\VignetteIndexEntry{2_smoothing_data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

<style type = "text/css">
h1.title {
  font-size: 30px;
}
h1 { /* Header 1 */
  font-size: 26px;
}
h2 { /* Header 2 */
    font-size: 18px;
}
h3 { /* Header 3 */
  font-size: 16px;
}
div#TOC li {
    list-style:none;
    background-image:none;
    background-repeat:none;
    background-position:0; 
}
</style>

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
options(rmarkdown.html_vignette.check_title = FALSE)

library(readr)
```

```{r setup, include = FALSE}
devtools::load_all()
#library(geocausal)
```

```{r load data, include = FALSE}
dat_hfr <- readRDS("~/geocausal/Data/dat_hfr_1.rds")
```

By the end of this vignette, users will be able to smooth the point process data and understand the usage of <tt>get_smoothed_outcome</tt> function.

# Smoothing outcomes

A key step of spatiotemporal causal inference is the smoothing of outcomes. To do so, we use <tt>get_smoothed_outcome</tt> function. This function has following arguments:

- <tt>data_interest</tt>: a hyperframe column that you want to convert; and
- <tt>method</tt>: a method of smoothing (either <tt>mclust</tt> or <tt>abram</tt>).

We have two methodological options for smoothing, which we will walk through below.

## Option 1: Gaussian mixture model

Let's first use the Gaussian mixture model, which is the simplest method for smoothing.

This method does the following:

- performing model-based clustering;
- obtaining the common variance (i.e., modeling all clusters with the common variance);
- using the common variance as the variance of the isotropic smoothing kernel; and
- performing smoothing of the point processes.

[Figure here]

The key advantage of choosing the Gaussian mixture model is its computational speed. Since our function assumes EII (equal volume, round shape), it is much faster than the adaptive smoothing.

To use this method, users first need to set <tt>method = "mclust"</tt> and then specify the <tt>initialization</tt> argument. By setting <tt>initialization = TRUE</tt>, the function uses a small fraction of data to obtain the common variance, which makes the entire process even faster.

For example, if you want to use 5% of data for initialization, then you can run the following code.

```{r}
smooth_IED <- get_smoothed_outcome(data_interest = dat_hfr$IED,
                                   method = "mclust", initialization = TRUE,
                                   sampling = 0.05)
```

It takes some time to complete this process, so it is highly recommended that you save the output.

Now, let's take a look at the output. it is a list of pixel images, each representing smoothed outcomes (in this case, IED) of each date.

```{r}
head(smooth_IED)
```

To visualize this, you can simply use <tt>plot</tt> function. For example, if you want to visualize the first day, you can run the following.

```{r, out.height=400, out.width=600, fig.dim = c(9, 6)}
plot(smooth_IED[[1]], main = "Smoothed IED (2007-02-23)")
```

Additionally, just as we did for the hyperframe, we can run <tt>vis_hfr</tt> function. To do so, save the output as a column of the hyperframe that we generated. Since we are visualizing pixel images, you can now specify the maximum value for scaling by specifying the argument <tt>scale_max</tt>.

```{r, out.height=400, out.width=600, fig.dim = c(9, 6)}
# Save the output as a column of the hyperframe
dat_hfr$smooth_IED <- smooth_IED

# Run vis_hfr function (combined, max scale = 200)
#vis_hfr(hfr = dat_hfr,
#        subtype_column = "smooth_IED",
#        time_column = "time",
#        range = c("2007-02-23"), "2007-02-28"),
#        combined = TRUE,
#        scale_max = 200)

# Run vis_hfr function (not combined, max scale = 50)
#vis_hfr(hfr = dat_hfr,
#        subtype_column = "smooth_IED",
#        time_column = "time",
#        range = c("2007-02-23", "2007-02-28"),
#        combined = FALSE,
#        scale_max = 50) 
```

Finally, we do the same for other types of outcomes and save each of them as a column of the hyperframe that we created.

```{r}
smooth_SAF <- get_smoothed_outcome(data_interest = dat_hfr$SAF,
                                   method = "mclust", initialization = TRUE,
                                   sampling = 0.05)

smooth_other <- get_smoothed_outcome(data_interest = dat_hfr$other_outcome,
                                     method = "mclust", initialization = TRUE,
                                     sampling = 0.05)

smooth_allout <- get_smoothed_outcome(data_interest = dat_hfr$all_outcome,
                                      method = "mclust", initialization = TRUE,
                                      sampling = 0.05)

# Save 
dat_hfr$smooth_SAF <- smooth_SAF
dat_hfr$smooth_other <- smooth_other
dat_hfr$smooth_allout <- smooth_allout
```

```{r, include = FALSE}
#saveRDS(dat_hfr, file = "~/geocausal/Data/dat_hfr_2.rds")
rm(smooth_IED, smooth_SAF, smooth_other, smooth_allout)
```

## Option 2: Abramson's adaptive smoothing

The Gaussian mixture model is a fixed-bandwidth smoothing approach. This approach, therefore, is problematic when the true intensity varies across regions because it over- or under-smoothes some regions. To incorporate the variation in intensity functions, we need to adaptively smooth point processes. 

Long stories short, we assume that the bandwidth is inversely proportional to the square root of the target densities. The function <tt>get_smoothed_outcome</tt> follows Abramson (1982) to estimate the target densities. Users can employ this option by setting <tt>method = "abram"</tt>.





