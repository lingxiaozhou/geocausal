---
title: "Step 2. Smoothing point process data"
output: 
  rmarkdown::html_vignette:
    toc: true
    number_sections: true
vignette: >
  %\VignetteIndexEntry{Step 2. Smoothing point process data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

<style type = "text/css">
h1.title {
  font-size: 30px;
}
h1 { /* Header 1 */
  font-size: 26px;
}
h2 { /* Header 2 */
    font-size: 18px;
}
h3 { /* Header 3 */
  font-size: 16px;
}
div#TOC li {
    list-style:none;
    background-image:none;
    background-repeat:none;
    background-position:0; 
}
</style>

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
options(rmarkdown.html_vignette.check_title = FALSE)
```

```{r setup, include = FALSE}
library(geocausal)
```

```{r load data, include = FALSE}
# Vignette 1 -----
airstrikes <- geocausal::airstrikes
insurgencies <- geocausal::insurgencies
iraq_window <- geocausal::iraq_window

# 1. Treatment data

## 1-1. Convert time variable to numerics
airstrikes$time <- as.numeric(airstrikes$date - min(airstrikes$date) + 1)

## 1-2. Generate a hyperframe
treatment_hfr <- get_hfr(data = airstrikes,
                         subtype_column = "type",
                         window = iraq_window,
                         time_column = "time",
                         time_range = c(1, max(airstrikes$time)),
                         coordinates = c("longitude", "latitude"),
                         combined = TRUE)

# 2. Outcome data

## 2-1. Convert time variable to numerics
insurgencies$time <- as.numeric(insurgencies$date - min(insurgencies$date) + 1)

outcome_hfr <- get_hfr(data = insurgencies,
                       subtype_column = "type",
                       window = iraq_window,
                       time_column = "time",
                       time_range = c(1, max(insurgencies$time)),
                       coordinates = c("longitude", "latitude"),
                       combined = TRUE)

# 3. Combine two hyperframes
dat_hfr <- spatstat.geom::cbind.hyperframe(treatment_hfr, outcome_hfr[, -1])
names(dat_hfr)[names(dat_hfr) == "all_combined"] <- "all_treatment"
names(dat_hfr)[names(dat_hfr) == "all_combined.1"] <- "all_outcome"
```

The second step in spatiotemporal causal inference involves smoothing outcomes. For this step, we use the `get_smoothed_outcome()` function.

# Smoothing Outcomes

The `get_smoothed_outcome()` function has two primary arguments. The first is `data_interest`, which refers to a column of the hyperframe that was generated in the first step. The second is `method`, which determines the smoothing method, either `mclust` or `abram`.

## Method 1: Gaussian Mixture Model (`method = mclust`)

The Gaussian mixture model offers a simple and swift approach to smoothing point processes. It performs model-based clustering of points by determining the common variance and then using this variance for the isotropic smoothing kernel. The primary advantage of opting for the Gaussian mixture model is its computational efficiency. Since our function utilizes the EII model (equal volume, round shape), the Gaussian mixture model is faster than adaptive smoothing (specified by `method = abram`).

To further accelerate this method, users can set `initialization = TRUE`. By doing this, the function will use only a fraction of the data to ascertain the common variance. For instance, to utilize 5% of the data for initialization, users can execute the provided code.

```{r}
# Smoothing
smooth_allout <- get_smoothed_outcome(data_interest = dat_hfr$all_outcome,
                                      method = "mclust", initialization = TRUE,
                                      sampling = 0.05)

# Save the output
dat_hfr$smooth_allout <- smooth_allout
```

Since the process is computationally demanding, users are highly recommended to save the output to avoid running the process multiple times. The output consists of a list of pixel images, each representing the smoothed outcomes for each time frame.

## Method 2: Abramson's Adaptive Smoothing (`method = abram`)

The Gaussian mixture model employs a fixed-bandwidth smoothing approach. This method can pose problems when the true intensity varies substantially across regions. The fixed-bandwidth approach may over-smooth or under-smooth the point process data in some areas. To account for the variation in intensity functions, it's preferable to use adaptive smoothing techniques.

The function `get_smoothed_outcome()` adopts Abramson (1982) to estimate target densities. The `geocausal` package assumes that the bandwidth is inversely proportional to the square root of the target densities. Users can apply the adaptive-bandwidth approach by setting `method = "abram"`.
