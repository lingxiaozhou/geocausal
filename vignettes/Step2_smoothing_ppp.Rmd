---
title: "Step 2. Smoothing point process data"
output: 
  rmarkdown::html_vignette:
    toc: true
    number_sections: true
vignette: >
  %\VignetteIndexEntry{Step 2. Smoothing point process data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

<style type = "text/css">
h1.title {
  font-size: 30px;
}
h1 { /* Header 1 */
  font-size: 26px;
}
h2 { /* Header 2 */
    font-size: 18px;
}
h3 { /* Header 3 */
  font-size: 16px;
}
div#TOC li {
    list-style:none;
    background-image:none;
    background-repeat:none;
    background-position:0; 
}
</style>

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
options(rmarkdown.html_vignette.check_title = FALSE)
```

```{r setup, include = FALSE}
library(geocausal)
```

```{r load data, include = FALSE}
# Vignette 1 -----
airstrikes <- geocausal::airstrikes
insurgencies <- geocausal::insurgencies
iraq_window <- geocausal::iraq_window

# 1. Treatment data

## 1-1. Convert time variable to numerics
airstrikes$time <- as.numeric(airstrikes$date - min(airstrikes$date) + 1)

## 1-2. Generate a hyperframe
treatment_hfr <- get_hfr(data = airstrikes,
                         subtype_column = "type",
                         window = iraq_window,
                         time_column = "time",
                         time_range = c(1, max(airstrikes$time)),
                         coordinates = c("longitude", "latitude"),
                         combined = TRUE)

# 2. Outcome data

## 2-1. Convert time variable to numerics
insurgencies$time <- as.numeric(insurgencies$date - min(insurgencies$date) + 1)

outcome_hfr <- get_hfr(data = insurgencies,
                       subtype_column = "type",
                       window = iraq_window,
                       time_column = "time",
                       time_range = c(1, max(insurgencies$time)),
                       coordinates = c("longitude", "latitude"),
                       combined = TRUE)

# 3. Combine two hyperframes
dat_hfr <- spatstat.geom::cbind.hyperframe(treatment_hfr, outcome_hfr[, -1])
names(dat_hfr)[names(dat_hfr) == "all_combined"] <- "all_treatment"
names(dat_hfr)[names(dat_hfr) == "all_combined.1"] <- "all_outcome"
```

The second step of spatiotemporal causal inference is smoothing outcomes. We employ `get_smoothed_outcome()` function to work on this step. 

# Smoothing outcomes

`get_smoothed_outcome()` function has two key arguments. The first is `data_interest`, which is a column of the hyperframe that we generated in the first step. The second is `method`, which specifies the method of smoothing (either `mclust` or `abram`).

## Method 1: Gaussian mixture model (`method = mclust`)

The Gaussian mixture model is the simplest and fastest method for smoothing point processes. This method performs model-based clustering of points by obtaining the common variance and using it as the variance of the isotropic smoothing kernel. The key advantage of choosing the Gaussian mixture model is its computational speed. Since our function employs the EII model (equal volume, round shape), the Gaussian mixture model is much faster than adaptive smoothing (`method = abram`).

To make this method even faster, users can specify `initialization = TRUE`. By doing so, the function uses a small portion of data to obtain the common variance. For example, in order to use 5% of data for initialization, then users can run the following code.

```{r}
# Smoothing
smooth_allout <- get_smoothed_outcome(data_interest = dat_hfr$all_outcome,
                                      method = "mclust", initialization = TRUE,
                                      sampling = 0.05)

# Save the output
dat_hfr$smooth_allout <- smooth_allout
```

Since the process is computationally demanding, it is highly recommended that users save the output and avoid running the process multiple times. The output is a list of pixel images, each representing smoothed outcomes of each time frame.

## Method 2: Abramson's adaptive smoothing (`method = abram`)

The Gaussian mixture model is a fixed-bandwidth smoothing approach. This approach, therefore, can be problematic when the true intensity varies substantively across regions because the fixed-bandwidth approach can over- or under-smooth point process data of some regions. To incorporate the variation in intensity functions, we need to employ adaptive smoothing techniques. 

The function `get_smoothed_outcome()` follows Abramson (1982) to estimate the target densities. `geocausal` package assumes that the bandwidth is inversely proportional to the square root of the target densities. Users can employ the adaptive-bandwidth approach by setting `method = "abram"`.
